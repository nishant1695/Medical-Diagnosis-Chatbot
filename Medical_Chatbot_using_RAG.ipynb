{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-ZUpUYaT_9"
      },
      "source": [
        "# Building a RAG application from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANQy2iUXaT__"
      },
      "source": [
        "Let's start by loading the environment variables we need to use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-8cF0BKkXaE",
        "outputId": "9429fafe-82d5-4039-e504-242768bfd165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 15))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-tvtoxq1c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-tvtoxq1c\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai (from -r requirements.txt (line 1))\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf (from -r requirements.txt (line 2))\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from -r requirements.txt (line 3))\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai (from -r requirements.txt (line 4))\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Collecting langchain_pinecone (from -r requirements.txt (line 5))\n",
            "  Downloading langchain_pinecone-0.0.3-py3-none-any.whl (8.3 kB)\n",
            "Collecting docarray (from -r requirements.txt (line 7))\n",
            "  Downloading docarray-0.40.0-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.8 (from -r requirements.txt (line 8))\n",
            "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytube (from -r requirements.txt (line 9))\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from -r requirements.txt (line 10))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting tiktoken (from -r requirements.txt (line 11))\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-client (from -r requirements.txt (line 12))\n",
            "  Downloading pinecone_client-3.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
            "Collecting ruff (from -r requirements.txt (line 14))\n",
            "  Downloading ruff-0.3.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.8->-r requirements.txt (line 8)) (4.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 1)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.29 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (8.2.3)\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.8.2 (from docarray->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray->-r requirements.txt (line 7)) (13.7.1)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray->-r requirements.txt (line 7))\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from docarray->-r requirements.txt (line 7))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r requirements.txt (line 11)) (2023.12.25)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 12)) (2024.2.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r requirements.txt (line 15)) (0.58.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r requirements.txt (line 15)) (2.1.0+cu121)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r requirements.txt (line 15)) (10.1.0)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r requirements.txt (line 15)) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain->-r requirements.txt (line 3))\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain->-r requirements.txt (line 3)) (3.20.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 3))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain->-r requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 7)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117->-r requirements.txt (line 15)) (3.13.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->docarray->-r requirements.txt (line 7))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117->-r requirements.txt (line 15)) (0.41.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 15)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray->-r requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117->-r requirements.txt (line 15)) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, hnswlib\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=79ea26a0036b1088f04682b52bd642bff0fcf1330759d0398daf3c24a20504fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tfcnokel/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287616 sha256=4ea8d66d795f52042acb9156e47d229546ab2f09064ab4682c7b9a4f85f25bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built openai-whisper hnswlib\n",
            "Installing collected packages: types-requests, ruff, pytube, python-dotenv, pypdf, pydantic, pinecone-client, orjson, mypy-extensions, marshmallow, jsonpointer, hnswlib, h11, typing-inspect, tiktoken, langsmith, jsonpatch, httpcore, openai-whisper, langchain-core, httpx, docarray, dataclasses-json, openai, langchain-text-splitters, langchain_pinecone, langchain-community, langchain-openai, langchain\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.3\n",
            "    Uninstalling pydantic-2.6.3:\n",
            "      Successfully uninstalled pydantic-2.6.3\n",
            "Successfully installed dataclasses-json-0.6.4 docarray-0.32.1 h11-0.14.0 hnswlib-0.8.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 langchain_pinecone-0.0.3 langsmith-0.1.22 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.13.3 openai-whisper-20231117 orjson-3.9.15 pinecone-client-3.1.0 pydantic-1.10.8 pypdf-4.1.0 python-dotenv-1.0.1 pytube-15.0.0 ruff-0.3.1 tiktoken-0.6.0 types-requests-2.31.0.20240218 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpACpccWaT__"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_ENV'] = userdata.get('PINECONE_API_ENV')\n",
        "\n",
        "\n",
        "\n",
        "# This is the YouTube video we're going to use.\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiGMU8RkaT__"
      },
      "source": [
        "## Setting up the model\n",
        "Let's define the LLM model that we'll use as part of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYUb51KnaT__"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key= userdata.get('OPENAI_API_KEY'), model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrOXcX2BaUAA"
      },
      "source": [
        "We can test the model by asking a simple question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcDWo7e2aUAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1b9a2c-d9fd-4ff0-b32e-1f2a8d682b6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1ooZp0HaUAA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae332115-344f-4dc8-c1b7-cf1450549a85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.output_parsers import\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wt2_qIoaUAA"
      },
      "source": [
        "## Introducing prompt templates\n",
        "\n",
        "We want to provide the model with some context and the question. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) are a simple way to define and reuse prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpqPtjG2aUAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "738ec35c-71da-4168-f9cd-d193127ddd20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nAnswer the question based on the context below. If you can\\'t\\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN9oyrOCaUAB"
      },
      "source": [
        "We can now chain the prompt with the model and the output parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7EUGseraUAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5dc2f188-47a1-4166-ecef-d4996c40c21d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Susana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"context\": \"Mary's sister is Susana\",\n",
        "    \"question\": \"Who is Mary's sister?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf\")\n",
        "transcription = loader.load()"
      ],
      "metadata": {
        "id": "DWlU0Hh04XHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dmHWQURaUAC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    chain.invoke({\n",
        "        \"context\": transcription,\n",
        "        \"question\": \"Is reading papers a good idea?\"\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-aJZkMaUAC"
      },
      "source": [
        "## Splitting the transcription\n",
        "\n",
        "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdOMMYdcaUAD"
      },
      "outputs": [],
      "source": [
        "'''from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()\n",
        "text_documents'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_BPdNEbaUAD"
      },
      "source": [
        "There are many different ways to split a document. For this example, we'll use a simple splitter that splits the document into chunks of a fixed size. Check [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) for more information about different approaches to splitting documents.\n",
        "\n",
        "For illustration purposes, let's split the transcription into chunks of 100 characters with an overlap of 20 characters and display the first few chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZSx2X0faUAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d5b7c0-870c-413b-fbd9-03f3e63a9912"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Encyclopedia of Intensive Care Medicine', metadata={'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf', 'page': 0}),\n",
              " Document(page_content='Jean-Louis Vincent and Jesse B. Hall (Eds)\\nEncyclopedia of\\nIntensive Care Medicine', metadata={'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf', 'page': 2}),\n",
              " Document(page_content='With 716 Figures and 450 Tables', metadata={'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf', 'page': 2}),\n",
              " Document(page_content='Editors\\nJean-Louis VincentHead Dept of Intensive CareErasme Hospital (Free University of Brussels)', metadata={'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf', 'page': 3}),\n",
              " Document(page_content='Route de Lennik 808\\n1070 BrusselsBelgium\\nJesse B. Hall', metadata={'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf', 'page': 3})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "text_splitter.split_documents(transcription)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMhTP_YGaUAD"
      },
      "source": [
        "For our specific application, let's use 1000 characters instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qMvt-E2aUAD"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQPJMVWCaUAD"
      },
      "source": [
        "## Finding the relevant chunks\n",
        "\n",
        "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of **embeddings** comes into play.\n",
        "\n",
        "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. We can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.\n",
        "\n",
        "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
        "\n",
        "Let's generate embeddings for an arbitrary query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoBfp_5VaUAD"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
        "\n",
        "print(f\"Embedding length: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4jrtlh_aUAD"
      },
      "source": [
        "To illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y22uiASOaUAE"
      },
      "outputs": [],
      "source": [
        "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
        "sentence2 = embeddings.embed_query(\"Pedro's mother is a teacher\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSx0aoPZaUAE"
      },
      "source": [
        "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
        "\n",
        "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XMwVyM7aUAE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
        "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
        "\n",
        "query_sentence1_similarity, query_sentence2_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGyH1EpiaUAE"
      },
      "source": [
        "## Setting up a Vector Store\n",
        "\n",
        "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
        "\n",
        "A vector store is a database of embeddings that specializes in fast similarity searches.\n",
        "\n",
        "\n",
        "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bylxIzeYaUAE"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Mary's sister is Susana\",\n",
        "        \"John and Tommy are brothers\",\n",
        "        \"Patricia likes white cars\",\n",
        "        \"Pedro's mother is a teacher\",\n",
        "        \"Lucia drives an Audi\",\n",
        "        \"Mary has two siblings\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cOB0uCtaUAL"
      },
      "source": [
        "We can now query the vector store to find the most similar embeddings to a given query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrhGFyAvaUAL"
      },
      "outputs": [],
      "source": [
        "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnczOSGyaUAL"
      },
      "source": [
        "## Connecting the vector store to the chain\n",
        "\n",
        "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
        "\n",
        "We need to configure a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
        "\n",
        "We can get a retriever directly from the vector store we created before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJYDv-7naUAM"
      },
      "outputs": [],
      "source": [
        "retriever1 = vectorstore1.as_retriever()\n",
        "retriever1.invoke(\"Who is Mary's sister?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68VC-VKEaUAM"
      },
      "source": [
        "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
        "\n",
        "We can create a map with the two inputs by using the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) and [`RunnablePassthrough`](https://python.langchain.com/docs/expression_language/how_to/passthrough) classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gec_9rO1aUAM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
        "setup.invoke(\"What color is Patricia's car?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSCTl-UCaUAM"
      },
      "source": [
        "Let's now add the setup map to the chain and run it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXKCgHSDaUAM"
      },
      "outputs": [],
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"What color is Patricia's car?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLG90ZNfaUAM"
      },
      "source": [
        "Let's invoke the chain using another example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLQUeYYtaUAM"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"What car does Lucia drive?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEoHFdjkaUAM"
      },
      "source": [
        "## Loading transcription into the vector store\n",
        "\n",
        "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwBne8byaUAN"
      },
      "source": [
        "Let's set up a new chain using the correct vector store. This time we are using a different equivalent syntax to specify the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) portion of the chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URC5oYrqaUAN"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is synthetic intelligence?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRLlPUdYaUAN"
      },
      "source": [
        "## Setting up Pinecone\n",
        "\n",
        "So far we've used an in-memory vector store. In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. For this example, we'll use [Pinecone](https://www.pinecone.io/).\n",
        "\n",
        "The first step is to create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n",
        "\n",
        "Then, we can load the transcription documents into Pinecone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABH6-G_haUAN"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "pinecone = PineconeVectorStore.from_documents(documents, embeddings, index_name = userdata.get('PINECONE_API_ENV'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxPFCwe3aUAN"
      },
      "source": [
        "Let's now run a similarity search on pinecone to make sure everything works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ISYUtNqaUAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa27b89d-381d-4b00-981c-ac3b1a10b56b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='muscle protein. Crit Care Med 35(9 Suppl):S630–S634\\nProtein-Energy Wasting\\n▶Metabolic Disorders, Other\\nPSB\\nProtected Specimen Brush (PSB).\\nPseudoaneurysm\\nSCOTT E. B ELL1,KATHRYN M. B EAUCHAMP2\\n1Department of Neurosurgery, School of Medicine,\\nUniversity of Colorado Health Sciences Center,\\nDenver, CO, USA\\n2Department of Neurosurgery, Denver Health Medical\\nCenter, University of Colorado School of Medicine,Denver, CO, USA\\nDefinition\\nPseudoaneurysm, or false aneurysm, describes a communi-\\ncation between a lumen and the surrounding soft tissue. It is\\ncaused by direct or indirect injury to a vessel, but may alsooccur with ischemic injury to the heart. This is a condition ofiatrogenic, traumatic, anastomotic, infected, or ischemic', metadata={'page': 1916.0, 'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf'}),\n",
              " Document(page_content='pseudoaneurysms. One dangerous etiology of pseudo-aneurysm arises from cardiac infarction, and carries ahigh degree of mortality compared to other causes.While occurring in just under 1% of M.I. cases [ 2], cardiac\\npseudoaneurysms are difﬁcult to detect and may gounnoticed until a sentinel event occurs. Despiteencompassing a low percentage of affected individuals,\\nthis still accounts for a prevalence of approximately\\n50,000–70,000 cases of cardiac pseudoaneurysm. Less fre-quent causes of pathologic false aneurysms include bluntor penetrating trauma, which can cause direct vesselinjury, or indirect injury by percussive effect in the tissues.Other causes arise as a complication of procedures forvascular disease including perforation of a vessel duringendovascular procedures, and leakage after vascular anas-\\ntomosis or vessel infection. Mycotic pseudoaneurysms\\noccur after bacteremia seeds an infection at a vessel thatthen goes on to erode and rupture.Pseudoaneurysm P 1865\\nP', metadata={'page': 1916.0, 'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf'}),\n",
              " Document(page_content='nonsurgical treatments for pseudoaneurysm is highly\\ndependent upon the patient’s use of anticoagulation.Surgical remediation remains the gold standard for thosedifﬁcult-to-access lesions. Endovascular treatment is gen-erally reserved for only select circumstances. With earlyidentiﬁcation and treatment, pseudoaneurysms have lowmorbidity and mortality. If delayed or unrecognized, these\\nlesions can be very dangerous to the patient. As more\\nendovascular procedures are performed, a conscious con-sideration of pseudoaneurysm formation is important forearly recognition and protection of the patient.\\nReferences\\n1. Kalapatapu VR, Shelton KR, Ali AT, Moursi MM, Eidt JF\\n(2008) Pseudoaneurysm: a review. Curr Treat Options Cardiovasc\\nMed 10:173–183\\n2. Frances C, Romero A, Grady D (1998) Left ventricular\\npseudoaneurysm. J Am Coll Cardiol 32:557–561\\n3. Stone PA, AbuRahma AF, Flaherty SK, Bates MC (2006) Femoral\\npseudoaneurysms. Vasc Endovasc Surg 40(2):109–117', metadata={'page': 1919.0, 'source': '/content/Jean-Louis Vincent, Jesse B. Hall (eds.) - Encyclopedia of Intensive Care Medicine-Springer-Verlag Berlin Heidelberg (2012).pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pinecone.similarity_search(\"What is pseudoaneurysm ?\")[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jusU4_wpaUAO"
      },
      "source": [
        "Let's setup the new chain using Pinecone as the vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e5_dghPaUAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad288c3-0658-4001-fc06-407cda9ff49a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='- Observation without treatment is reasonable for pseudoaneurysms <3 cm in size, non-expansile, non-painful, and without post-procedure anticoagulation\\n- Surgical remediation is the gold standard for difficult-to-access lesions\\n- Endovascular treatment is reserved for select circumstances\\n- Early identification and treatment of pseudoaneurysms is crucial to prevent complications\\n- Use of anticoagulation by the patient is important for nonsurgical treatments\\n- Activity restrictions post-treatment are recommended to allow for proper healing and prevent complications')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "chain = (\n",
        "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke(\"Give me a set of clinical guidelines to treat a pseudoaneurysm\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}